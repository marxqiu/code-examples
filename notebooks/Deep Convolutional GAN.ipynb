{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Convolutional GAN (DCGAN) \n",
        "For the first part of this assignment, we will implement a Deep Convolutional GAN (DCGAN).\n",
        "A DCGAN is simply a GAN that uses a convolutional neural network as the discriminator, and\n",
        "a network composed of transposed convolutions as the generator. To implement the DCGAN, we\n",
        "need to specify three things: 1) the generator, 2) the discriminator, and 3) the training procedure.\n",
        "We will go over each of these three components in the following subsections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "# Setup PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-6MQhMOlHXD",
        "outputId": "744ec303-792e-43e2-eb3a-b52b1de5624a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio) (1.21.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "/content/csc413/a4\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install imageio\n",
        "\n",
        "!pip install matplotlib\n",
        "\n",
        "%mkdir -p /content/csc413/a4/\n",
        "%cd /content/csc413/a4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "\n",
        "import imageio\n",
        "from urllib.error import URLError\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "                \n",
        "def to_var(tensor, cuda=True):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "    \n",
        "def to_data(x):\n",
        "    \"\"\"Converts variable to numpy.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cpu()\n",
        "    return x.data.numpy()\n",
        "\n",
        "\n",
        "def create_dir(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def gan_checkpoint(iteration, G, D, opts):\n",
        "    \"\"\"Saves the parameters of the generator G and discriminator D.\n",
        "    \"\"\"\n",
        "    G_path = os.path.join(opts.checkpoint_dir, 'G.pkl')\n",
        "    D_path = os.path.join(opts.checkpoint_dir, 'D.pkl')\n",
        "    torch.save(G.state_dict(), G_path)\n",
        "    torch.save(D.state_dict(), D_path)\n",
        "\n",
        "def load_checkpoint(opts):\n",
        "    \"\"\"Loads the generator and discriminator models from checkpoints.\n",
        "    \"\"\"\n",
        "    G_path = os.path.join(opts.load, 'G.pkl')\n",
        "    D_path = os.path.join(opts.load, 'D_.pkl')\n",
        "\n",
        "    G = DCGenerator(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim, spectral_norm=opts.spectral_norm)\n",
        "    D = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
        "\n",
        "    G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
        "    D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        G.cuda()\n",
        "        D.cuda()\n",
        "        print('Models moved to GPU.')\n",
        "\n",
        "    return G, D\n",
        "\n",
        "\n",
        "def merge_images(sources, targets, opts):\n",
        "    \"\"\"Creates a grid consisting of pairs of columns, where the first column in\n",
        "    each pair contains images source images and the second column in each pair\n",
        "    contains images generated by the CycleGAN from the corresponding images in\n",
        "    the first column.\n",
        "    \"\"\"\n",
        "    _, _, h, w = sources.shape\n",
        "    row = int(np.sqrt(opts.batch_size))\n",
        "    merged = np.zeros([3, row * h, row * w * 2])\n",
        "    for (idx, s, t) in (zip(range(row ** 2), sources, targets, )):\n",
        "        i = idx // row\n",
        "        j = idx % row\n",
        "        merged[:, i * h:(i + 1) * h, (j * 2) * h:(j * 2 + 1) * h] = s\n",
        "        merged[:, i * h:(i + 1) * h, (j * 2 + 1) * h:(j * 2 + 2) * h] = t\n",
        "    return merged.transpose(1, 2, 0)\n",
        "\n",
        "\n",
        "def generate_gif(directory_path, keyword=None):\n",
        "    images = []\n",
        "    for filename in sorted(os.listdir(directory_path)):\n",
        "        if filename.endswith(\".png\") and (keyword is None or keyword in filename):\n",
        "            img_path = os.path.join(directory_path, filename)\n",
        "            print(\"adding image {}\".format(img_path))\n",
        "            images.append(imageio.imread(img_path))\n",
        "\n",
        "    if keyword:\n",
        "        imageio.mimsave(\n",
        "            os.path.join(directory_path, 'anim_{}.gif'.format(keyword)), images)\n",
        "    else:\n",
        "        imageio.mimsave(os.path.join(directory_path, 'anim.gif'), images)\n",
        "\n",
        "\n",
        "def create_image_grid(array, ncols=None):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    num_images, channels, cell_h, cell_w = array.shape\n",
        "    if not ncols:\n",
        "        ncols = int(np.sqrt(num_images))\n",
        "    nrows = int(np.math.floor(num_images / float(ncols)))\n",
        "    result = np.zeros((cell_h * nrows, cell_w * ncols, channels), dtype=array.dtype)\n",
        "    for i in range(0, nrows):\n",
        "        for j in range(0, ncols):\n",
        "            result[i * cell_h:(i + 1) * cell_h, j * cell_w:(j + 1) * cell_w, :] = array[i * ncols + j].transpose(1, 2,\n",
        "                                                                                                                 0)\n",
        "\n",
        "    if channels == 1:\n",
        "        result = result.squeeze()\n",
        "    return result\n",
        "\n",
        "\n",
        "def gan_save_samples(G, fixed_noise, iteration, opts):\n",
        "    generated_images = G(fixed_noise)\n",
        "    generated_images = to_data(generated_images)\n",
        "\n",
        "    grid = create_image_grid(generated_images)\n",
        "\n",
        "    # merged = merge_images(X, fake_Y, opts)\n",
        "    path = os.path.join(opts.sample_dir, 'sample-{:06d}.png'.format(iteration))\n",
        "    imageio.imwrite(path, grid)\n",
        "    print('Saved {}'.format(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "outputs": [],
      "source": [
        "def get_emoji_loader(emoji_type, opts):\n",
        "    \"\"\"Creates training and test data loaders.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "                    transforms.Scale(opts.image_size),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                ])\n",
        "\n",
        "    train_path = os.path.join('data/emojis', emoji_type)\n",
        "    test_path = os.path.join('data/emojis', 'Test_{}'.format(emoji_type))\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
        "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
        "\n",
        "    train_dloader = DataLoader(dataset=train_dataset, batch_size=opts.batch_size, shuffle=True, num_workers=opts.num_workers)\n",
        "    test_dloader = DataLoader(dataset=test_dataset, batch_size=opts.batch_size, shuffle=False, num_workers=opts.num_workers)\n",
        "\n",
        "    return train_dloader, test_dloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nSIhQp41q_Nu"
      },
      "outputs": [],
      "source": [
        "def print_models(G_XtoY, G_YtoX, D_X, D_Y):\n",
        "    \"\"\"Prints model information for the generators and discriminators.\n",
        "    \"\"\"\n",
        "    print(\"                 G                     \")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(G_XtoY)\n",
        "    print(\"---------------------------------------\")\n",
        "\n",
        "    print(\"                  D                    \")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(D_X)\n",
        "    print(\"---------------------------------------\")\n",
        "\n",
        "\n",
        "def create_model(opts):\n",
        "    \"\"\"Builds the generators and discriminators.\n",
        "    \"\"\"\n",
        "    ### GAN\n",
        "    G = DCGenerator(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim, spectral_norm=opts.spectral_norm)\n",
        "    D = DCDiscriminator(conv_dim=opts.d_conv_dim, spectral_norm=opts.spectral_norm)\n",
        "\n",
        "    print_models(G, None, D, None)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        G.cuda()\n",
        "        D.cuda()\n",
        "        print('Models moved to GPU.')\n",
        "    return G, D\n",
        "\n",
        "def train(opts):\n",
        "    \"\"\"Loads the data, creates checkpoint and sample directories, and starts the training loop.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create train and test dataloaders for images from the two domains X and Y\n",
        "    dataloader_X, test_dataloader_X = get_emoji_loader(emoji_type=opts.X, opts=opts)\n",
        "    \n",
        "    # Create checkpoint and sample directories\n",
        "    create_dir(opts.checkpoint_dir)\n",
        "    create_dir(opts.sample_dir)\n",
        "\n",
        "    # Start training\n",
        "    if opts.least_squares_gan:\n",
        "        G, D = gan_training_loop_leastsquares(dataloader_X, test_dataloader_X, opts)\n",
        "    else:\n",
        "        G, D = gan_training_loop_regular(dataloader_X, test_dataloader_X, opts)\n",
        "        \n",
        "    return G, D\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        if opts.__dict__[key]:\n",
        "            print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0KxX0sDpXKb"
      },
      "source": [
        "## Helper modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "y7s0etAmpUgT"
      },
      "outputs": [],
      "source": [
        "def sample_noise(batch_size, dim):\n",
        "    \"\"\"\n",
        "    Generate a PyTorch Tensor of uniform random noise.\n",
        "\n",
        "    Input:\n",
        "    - batch_size: Integer giving the batch size of noise to generate.\n",
        "    - dim: Integer giving the dimension of noise to generate.\n",
        "\n",
        "    Output:\n",
        "    - A PyTorch Tensor of shape (batch_size, dim, 1, 1) containing uniform\n",
        "      random noise in the range (-1, 1).\n",
        "    \"\"\"\n",
        "    return to_var(torch.rand(batch_size, dim) * 2 - 1).unsqueeze(2).unsqueeze(3)\n",
        "  \n",
        "\n",
        "def upconv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, spectral_norm=False):\n",
        "    \"\"\"Creates a upsample-and-convolution layer, with optional batch normalization.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    if stride>1:\n",
        "        layers.append(nn.Upsample(scale_factor=stride))\n",
        "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)\n",
        "    if spectral_norm:\n",
        "        layers.append(SpectralNorm(conv_layer))\n",
        "    else:\n",
        "        layers.append(conv_layer)\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, init_zero_weights=False, spectral_norm=False):\n",
        "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "    if init_zero_weights:\n",
        "        conv_layer.weight.data = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n",
        "            \n",
        "    if spectral_norm:\n",
        "        layers.append(SpectralNorm(conv_layer))\n",
        "    else:\n",
        "        layers.append(conv_layer)\n",
        "\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, conv_dim):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_layer = conv(in_channels=conv_dim, out_channels=conv_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_layer(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0_YbBwe5k35"
      },
      "source": [
        "## DCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1E_jDaBLT1P"
      },
      "source": [
        "### Spectral Norm class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2hy97i1-LT1Q"
      },
      "outputs": [],
      "source": [
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BAfi_8yWB3y"
      },
      "source": [
        "### GAN generator\n",
        "Now, we will implement the generator of the DCGAN, which consists of a sequence of transpose\n",
        "convolutional layers that progressively upsample the input noise sample to generate a fake image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9ztmyA5Ro67o"
      },
      "outputs": [],
      "source": [
        "class DCGenerator(nn.Module):\n",
        "    def __init__(self, noise_size, conv_dim, spectral_norm=False):\n",
        "        super(DCGenerator, self).__init__()\n",
        "\n",
        "        self.conv_dim = conv_dim\n",
        "\n",
        "        self.linear_bn = nn.Sequential(\n",
        "            nn.Linear(noise_size, self.conv_dim*4*4*4),\n",
        "            nn.BatchNorm1d(self.conv_dim*4*4*4)\n",
        "            )\n",
        "        self.upconv1 = upconv(in_channels=self.conv_dim*4, out_channels=self.conv_dim*2, kernel_size=5, stride=2, padding=2, batch_norm=True, spectral_norm=spectral_norm)\n",
        "        self.upconv2 = upconv(in_channels=self.conv_dim*2, out_channels=self.conv_dim, kernel_size=5, stride=2, padding=2, batch_norm=True, spectral_norm=spectral_norm)\n",
        "        self.upconv3 = upconv(in_channels=self.conv_dim, out_channels=3, kernel_size=5, stride=2, padding=2, batch_norm=False, spectral_norm=spectral_norm)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"Generates an image given a sample of random noise.\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "                z: BS x noise_size x 1 x 1   -->  BSx100x1x1 (during training)\n",
        "\n",
        "            Output\n",
        "            ------\n",
        "                out: BS x channels x image_width x image_height  -->  BSx3x32x32 (during training)\n",
        "        \"\"\"\n",
        "        batch_size = z.size(0)\n",
        "        \n",
        "        z = z.view(batch_size, -1)\n",
        "        out = F.relu(self.linear_bn(z)).view(-1, self.conv_dim*4, 4, 4)    # BS x 128 x 4 x 4\n",
        "        out = F.relu(self.upconv1(out))  # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.upconv2(out))  # BS x 32 x 16 x 16\n",
        "        out = F.tanh(self.upconv3(out))  # BS x 3 x 32 x 32\n",
        "        \n",
        "        out_size = out.size()\n",
        "        if out_size != torch.Size([batch_size, 3, 32, 32]):\n",
        "            raise ValueError(\"expect {} x 3 x 32 x 32, but get {}\".format(batch_size, out_size))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG4uqAVPp8_B"
      },
      "source": [
        "### GAN discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0GkjXydnqARR"
      },
      "outputs": [],
      "source": [
        "class DCDiscriminator(nn.Module):\n",
        "    \"\"\"Defines the architecture of the discriminator network.\n",
        "       Note: Both discriminators D_X and D_Y have the same architecture in this assignment.\n",
        "    \"\"\"\n",
        "    def __init__(self, conv_dim=64, spectral_norm=False):\n",
        "        super(DCDiscriminator, self).__init__()\n",
        "\n",
        "\n",
        "        self.conv1 = conv(in_channels=3, out_channels=conv_dim, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
        "        self.conv2 = conv(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
        "        self.conv3 = conv(in_channels=conv_dim*2, out_channels=conv_dim*4, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
        "        self.conv4 = conv(in_channels=conv_dim*4, out_channels=1, kernel_size=5, stride=2, padding=1, batch_norm=False, spectral_norm=spectral_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        out = F.relu(self.conv1(x))    # BS x 64 x 16 x 16\n",
        "        out = F.relu(self.conv2(out))    # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.conv3(out))    # BS x 64 x 4 x 4\n",
        "\n",
        "        out = self.conv4(out).squeeze()\n",
        "        out_size = out.size()\n",
        "        if out_size != torch.Size([batch_size,]):\n",
        "            raise ValueError(\"expect {} x 1, but get {}\".format(batch_size, out_size))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8RtBMu55ysm"
      },
      "source": [
        "### GAN training loop \n",
        "Next, you will implement the training loop for the DCGAN. A DCGAN is simply a GAN with a\n",
        "specific type of generator and discriminator; thus, we train it in exactly the same way as a standard\n",
        "GAN. \n",
        "\n",
        "*   Regular GAN\n",
        "*   Least Squares GAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MxIJ2Zua51KI"
      },
      "outputs": [],
      "source": [
        "def gan_training_loop_regular(dataloader, test_dataloader, opts):\n",
        "    \"\"\"Runs the training loop.\n",
        "        * Saves checkpoint every opts.checkpoint_every iterations\n",
        "        * Saves generated samples every opts.sample_every iterations\n",
        "    \"\"\"\n",
        "\n",
        "    # Create generators and discriminators\n",
        "    G, D = create_model(opts)\n",
        "\n",
        "    g_params = G.parameters()  # Get generator parameters\n",
        "    d_params = D.parameters()  # Get discriminator parameters\n",
        "\n",
        "    # Create optimizers for the generators and discriminators\n",
        "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "    d_optimizer = optim.Adam(d_params, opts.lr * 2., [opts.beta1, opts.beta2])\n",
        "\n",
        "    train_iter = iter(dataloader)\n",
        "\n",
        "    test_iter = iter(test_dataloader)\n",
        "\n",
        "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
        "    # constant throughout training, that allow us to inspect the model's performance.\n",
        "    fixed_noise = sample_noise(100, opts.noise_size)  # # 100 x noise_size x 1 x 1\n",
        "\n",
        "    iter_per_epoch = len(train_iter)\n",
        "    total_train_iters = opts.train_iters\n",
        "\n",
        "    losses = {\"iteration\": [], \"D_fake_loss\": [], \"D_real_loss\": [], \"G_loss\": []}\n",
        "    \n",
        "    gp_weight = 1\n",
        "\n",
        "    adversarial_loss = torch.nn.BCEWithLogitsLoss() # Use this loss\n",
        "    # [Hint: you may find the folowing code helpful]\n",
        "    #ones = Variable(torch.Tensor(real_images.shape[0]).float().cuda().fill_(1.0), requires_grad=False)\n",
        "\n",
        "    try:\n",
        "        for iteration in range(1, opts.train_iters + 1):\n",
        "\n",
        "            # Reset data_iter for each epoch\n",
        "            if iteration % iter_per_epoch == 0:\n",
        "                train_iter = iter(dataloader)\n",
        "\n",
        "            real_images, real_labels = train_iter.next()\n",
        "            real_images, real_labels = to_var(real_images), to_var(real_labels).long().squeeze()\n",
        "            \n",
        "\n",
        "            for d_i in range(opts.d_train_iters):\n",
        "                d_optimizer.zero_grad()\n",
        "\n",
        "                # 1. Compute the discriminator loss on real images\n",
        "                ones = Variable(torch.Tensor(real_images.shape[0]).float().cuda().fill_(1.0), requires_grad=False)\n",
        "                D_real_loss = adversarial_loss(D(real_images), ones)\n",
        "\n",
        "                # 2. Sample noise\n",
        "                noise = sample_noise(real_images.shape[0], opts.noise_size)\n",
        "\n",
        "                # 3. Generate fake images from the noise\n",
        "                fake_images = G(noise)     \n",
        "\n",
        "                # 4. Compute the discriminator loss on the fake images\n",
        "                zeros = Variable(torch.Tensor(real_images.shape[0]).float().cuda().fill_(0.0), requires_grad=False)\n",
        "                D_fake_loss = adversarial_loss(D(fake_images), zeros)\n",
        "\n",
        "                # ---- Gradient Penalty ----\n",
        "                if opts.gradient_penalty:\n",
        "                    alpha = torch.rand(real_images.shape[0], 1, 1, 1)\n",
        "                    alpha = alpha.expand_as(real_images).cuda()\n",
        "                    interp_images = Variable(alpha * real_images.data + (1 - alpha) * fake_images.data, requires_grad=True).cuda()\n",
        "                    D_interp_output = D(interp_images)\n",
        "\n",
        "                    gradients = torch.autograd.grad(outputs=D_interp_output, inputs=interp_images,\n",
        "                                                    grad_outputs=torch.ones(D_interp_output.size()).cuda(),\n",
        "                                                    create_graph=True, retain_graph=True)[0]\n",
        "                    gradients = gradients.view(real_images.shape[0], -1)\n",
        "                    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "\n",
        "                    gp = gp_weight * gradients_norm.mean()\n",
        "                else:\n",
        "                    gp = 0.0\n",
        "\n",
        "                # --------------------------\n",
        "                # 5. Compute the total discriminator loss\n",
        "                D_total_loss = D_real_loss + D_fake_loss + gp\n",
        "\n",
        "                D_total_loss.backward()\n",
        "                d_optimizer.step()\n",
        "\n",
        "            ###########################################\n",
        "            ###          TRAIN THE GENERATOR        ###\n",
        "            ###########################################\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            # 1. Sample noise\n",
        "            noise = sample_noise(real_images.shape[0], opts.noise_size)\n",
        "\n",
        "\n",
        "            # 2. Generate fake images from the noise\n",
        "            fake_images = G(noise)     \n",
        "\n",
        "            # 3. Compute the generator loss\n",
        "            ones = Variable(torch.Tensor(real_images.shape[0]).float().cuda().fill_(1.0), requires_grad=False)\n",
        "            G_loss = adversarial_loss(D(fake_images), ones)\n",
        "\n",
        "            G_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            # Print the log info\n",
        "            if iteration % opts.log_step == 0:\n",
        "                losses['iteration'].append(iteration)\n",
        "                losses['D_real_loss'].append(D_real_loss.item())\n",
        "                losses['D_fake_loss'].append(D_fake_loss.item())\n",
        "                losses['G_loss'].append(G_loss.item())\n",
        "                print('Iteration [{:4d}/{:4d}] | D_real_loss: {:6.4f} | D_fake_loss: {:6.4f} | G_loss: {:6.4f}'.format(\n",
        "                    iteration, total_train_iters, D_real_loss.item(), D_fake_loss.item(), G_loss.item()))\n",
        "\n",
        "            # Save the generated samples\n",
        "            if iteration % opts.sample_every == 0:\n",
        "                gan_save_samples(G, fixed_noise, iteration, opts)\n",
        "\n",
        "            # Save the model parameters\n",
        "            if iteration % opts.checkpoint_every == 0:\n",
        "                gan_checkpoint(iteration, G, D, opts)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return G, D\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(losses['iteration'], losses['D_real_loss'], label='D_real')\n",
        "    plt.plot(losses['iteration'], losses['D_fake_loss'], label='D_fake')\n",
        "    plt.plot(losses['iteration'], losses['G_loss'], label='G')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(opts.sample_dir, 'losses.png'))\n",
        "    plt.close()\n",
        "    return G, D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cCla1-2mezfn"
      },
      "outputs": [],
      "source": [
        "def gan_training_loop_leastsquares(dataloader, test_dataloader, opts):\n",
        "    \"\"\"Runs the training loop.\n",
        "        * Saves checkpoint every opts.checkpoint_every iterations\n",
        "        * Saves generated samples every opts.sample_every iterations\n",
        "    \"\"\"\n",
        "\n",
        "    # Create generators and discriminators\n",
        "    G, D = create_model(opts)\n",
        "\n",
        "    g_params = G.parameters()  # Get generator parameters\n",
        "    d_params = D.parameters()  # Get discriminator parameters\n",
        "\n",
        "    # Create optimizers for the generators and discriminators\n",
        "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "    d_optimizer = optim.Adam(d_params, opts.lr * 2., [opts.beta1, opts.beta2])\n",
        "\n",
        "    train_iter = iter(dataloader)\n",
        "\n",
        "    test_iter = iter(test_dataloader)\n",
        "\n",
        "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
        "    # constant throughout training, that allow us to inspect the model's performance.\n",
        "    fixed_noise = sample_noise(100, opts.noise_size)  # # 100 x noise_size x 1 x 1\n",
        "\n",
        "    iter_per_epoch = len(train_iter)\n",
        "    total_train_iters = opts.train_iters\n",
        "\n",
        "    losses = {\"iteration\": [], \"D_fake_loss\": [], \"D_real_loss\": [], \"G_loss\": []}\n",
        "\n",
        "    #adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
        "    gp_weight = 1\n",
        "\n",
        "    try:\n",
        "        for iteration in range(1, opts.train_iters + 1):\n",
        "\n",
        "            # Reset data_iter for each epoch\n",
        "            if iteration % iter_per_epoch == 0:\n",
        "                train_iter = iter(dataloader)\n",
        "\n",
        "            real_images, real_labels = train_iter.next()\n",
        "            real_images, real_labels = to_var(real_images), to_var(real_labels).long().squeeze()\n",
        "\n",
        "\n",
        "            for d_i in range(opts.d_train_iters):\n",
        "                d_optimizer.zero_grad()\n",
        "\n",
        "                # 1. Compute the discriminator loss on real images\n",
        "                D_real_loss = torch.mean((D(real_images) - 1) ** 2) / 2\n",
        "\n",
        "                # 2. Sample noise\n",
        "                noise = sample_noise(real_images.shape[0], opts.noise_size)\n",
        "\n",
        "                # 3. Generate fake images from the noise\n",
        "                fake_images = G(noise)\n",
        "\n",
        "                # 4. Compute the discriminator loss on the fake images\n",
        "                D_fake_loss = torch.mean(D(fake_images) ** 2) / 2\n",
        "\n",
        "                # ---- Gradient Penalty ----\n",
        "                if opts.gradient_penalty:\n",
        "                    alpha = torch.rand(real_images.shape[0], 1, 1, 1)\n",
        "                    alpha = alpha.expand_as(real_images).cuda()\n",
        "                    interp_images = Variable(alpha * real_images.data + (1 - alpha) * fake_images.data, requires_grad=True).cuda()\n",
        "                    D_interp_output = D(interp_images)\n",
        "\n",
        "                    gradients = torch.autograd.grad(outputs=D_interp_output, inputs=interp_images,\n",
        "                                                    grad_outputs=torch.ones(D_interp_output.size()).cuda(),\n",
        "                                                    create_graph=True, retain_graph=True)[0]\n",
        "                    gradients = gradients.view(real_images.shape[0], -1)\n",
        "                    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "\n",
        "                    gp = gp_weight * gradients_norm.mean()\n",
        "                else:\n",
        "                    gp = 0.0\n",
        "\n",
        "                # --------------------------\n",
        "                # 5. Compute the total discriminator loss\n",
        "                D_total_loss = D_real_loss + D_fake_loss + gp\n",
        "\n",
        "                D_total_loss.backward()\n",
        "                d_optimizer.step()\n",
        "\n",
        "            ###########################################\n",
        "            ###          TRAIN THE GENERATOR        ###\n",
        "            ###########################################\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            # 1. Sample noise\n",
        "            noise = sample_noise(real_images.shape[0], opts.noise_size)\n",
        "\n",
        "            # 2. Generate fake images from the noise\n",
        "            fake_images = G(noise)\n",
        "\n",
        "            # 3. Compute the generator loss\n",
        "            G_loss = torch.mean((D(fake_images) - 1)**2)\n",
        "\n",
        "            G_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            # Print the log info\n",
        "            if iteration % opts.log_step == 0:\n",
        "                losses['iteration'].append(iteration)\n",
        "                losses['D_real_loss'].append(D_real_loss.item())\n",
        "                losses['D_fake_loss'].append(D_fake_loss.item())\n",
        "                losses['G_loss'].append(G_loss.item())\n",
        "                print('Iteration [{:4d}/{:4d}] | D_real_loss: {:6.4f} | D_fake_loss: {:6.4f} | G_loss: {:6.4f}'.format(\n",
        "                    iteration, total_train_iters, D_real_loss.item(), D_fake_loss.item(), G_loss.item()))\n",
        "\n",
        "            # Save the generated samples\n",
        "            if iteration % opts.sample_every == 0:\n",
        "                gan_save_samples(G, fixed_noise, iteration, opts)\n",
        "\n",
        "            # Save the model parameters\n",
        "            if iteration % opts.checkpoint_every == 0:\n",
        "                gan_checkpoint(iteration, G, D, opts)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return G, D\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(losses['iteration'], losses['D_real_loss'], label='D_real')\n",
        "    plt.plot(losses['iteration'], losses['D_fake_loss'], label='D_fake')\n",
        "    plt.plot(losses['iteration'], losses['G_loss'], label='G')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(opts.sample_dir, 'losses.png'))\n",
        "    plt.close()\n",
        "    return G, D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNFd6LNo0-o"
      },
      "source": [
        "#  Training\n",
        "We will train a DCGAN to generate Windows (or Apple) emojis in the Training - GAN\n",
        "section of the notebook. By default, the script runs for 20000 iterations, and should take\n",
        "approximately 20 minutes on Colab. The script saves the output of the generator for a fixed\n",
        "noise sample every 200 iterations throughout training; this allows you to see how the generator\n",
        "improves over time. How does the generator performance evolve over time?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiUwiOITHTW4"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwcFjsEpHRbI",
        "outputId": "018c2e89-6773-4259-b75b-5d8ca91afde2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/emojis.tar.gz\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='emojis', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/emojis.tar.gz', \n",
        "                         untar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQmyJDSRFKR"
      },
      "source": [
        "## Train DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LKaRF1jwhH7",
        "outputId": "72e679cb-879c-441d-832c-0bba9b57e463",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                             image_size: 32                                     \n",
            "                             g_conv_dim: 32                                     \n",
            "                             d_conv_dim: 64                                     \n",
            "                             noise_size: 100                                    \n",
            "                            train_iters: 20000                                  \n",
            "                                      X: Apple                                  \n",
            "                                     lr: 3e-05                                  \n",
            "                                  beta1: 0.5                                    \n",
            "                                  beta2: 0.999                                  \n",
            "                             batch_size: 32                                     \n",
            "                       checkpoint_dir: results/checkpoints_gan_gp1_lr3e-5       \n",
            "                             sample_dir: results/samples_gan_gp1_lr3e-5         \n",
            "                               log_step: 200                                    \n",
            "                           sample_every: 200                                    \n",
            "                       checkpoint_every: 1000                                   \n",
            "                       gradient_penalty: 1                                      \n",
            "                          d_train_iters: 1                                      \n",
            "================================================================================\n",
            "                 G                     \n",
            "---------------------------------------\n",
            "DCGenerator(\n",
            "  (linear_bn): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=2048, bias=True)\n",
            "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (upconv1): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (upconv2): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (upconv3): Sequential(\n",
            "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (1): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "  )\n",
            ")\n",
            "---------------------------------------\n",
            "                  D                    \n",
            "---------------------------------------\n",
            "DCDiscriminator(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(256, 1, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  )\n",
            ")\n",
            "---------------------------------------\n",
            "Models moved to GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:317: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.981586217880249, 0.998660683631897]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [ 200/20000] | D_real_loss: 0.5276 | D_fake_loss: 0.5872 | G_loss: 0.8368\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-000200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9854655861854553, 0.9999748468399048]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [ 400/20000] | D_real_loss: 0.5268 | D_fake_loss: 0.4603 | G_loss: 1.0976\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-000400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9911456108093262, 0.9995404481887817]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [ 600/20000] | D_real_loss: 0.3438 | D_fake_loss: 0.4333 | G_loss: 1.2241\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-000600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9978124499320984, 0.999568521976471]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [ 800/20000] | D_real_loss: 0.3821 | D_fake_loss: 0.3998 | G_loss: 1.1957\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-000800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9984695911407471, 0.9999537467956543]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [1000/20000] | D_real_loss: 0.2534 | D_fake_loss: 0.2889 | G_loss: 1.4648\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-001000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9987400770187378, 0.9999687671661377]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [1200/20000] | D_real_loss: 0.3146 | D_fake_loss: 0.3092 | G_loss: 1.3229\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-001200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.997218906879425, 0.9999628663063049]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [1400/20000] | D_real_loss: 0.2755 | D_fake_loss: 0.2316 | G_loss: 1.3743\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-001400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9989630579948425, 0.9999751448631287]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [1600/20000] | D_real_loss: 0.1762 | D_fake_loss: 0.3815 | G_loss: 1.6584\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-001600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.99729323387146, 0.999973475933075]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [1800/20000] | D_real_loss: 0.2421 | D_fake_loss: 0.3011 | G_loss: 1.5041\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-001800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9970418214797974, 0.9999408721923828]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [2000/20000] | D_real_loss: 0.2484 | D_fake_loss: 0.2007 | G_loss: 1.8569\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-002000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9927131533622742, 0.9999798536300659]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [2200/20000] | D_real_loss: 0.1537 | D_fake_loss: 0.3181 | G_loss: 1.5188\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-002200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9943836331367493, 0.9999601244926453]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [2400/20000] | D_real_loss: 0.2246 | D_fake_loss: 0.2470 | G_loss: 1.6360\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-002400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9940105676651001, 0.9999268054962158]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [2600/20000] | D_real_loss: 0.3193 | D_fake_loss: 0.2360 | G_loss: 1.3802\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-002600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9969569444656372, 0.9999108910560608]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [2800/20000] | D_real_loss: 0.2747 | D_fake_loss: 0.1375 | G_loss: 1.7899\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-002800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9902913570404053, 0.9999141097068787]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [3000/20000] | D_real_loss: 0.1539 | D_fake_loss: 0.1982 | G_loss: 1.9283\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-003000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9922608137130737, 0.9998700618743896]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [3200/20000] | D_real_loss: 0.2134 | D_fake_loss: 0.2441 | G_loss: 1.4166\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-003200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9966014623641968, 0.999870240688324]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [3400/20000] | D_real_loss: 0.1250 | D_fake_loss: 0.1867 | G_loss: 2.0057\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-003400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9942688345909119, 0.9999178051948547]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [3600/20000] | D_real_loss: 0.1701 | D_fake_loss: 0.1792 | G_loss: 2.0904\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-003600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.995894193649292, 0.9998570084571838]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [3800/20000] | D_real_loss: 0.2237 | D_fake_loss: 0.1843 | G_loss: 1.5472\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-003800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9974918365478516, 0.9999552369117737]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [4000/20000] | D_real_loss: 0.1015 | D_fake_loss: 0.1762 | G_loss: 2.0454\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-004000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9971871972084045, 0.9999737739562988]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [4200/20000] | D_real_loss: 0.2456 | D_fake_loss: 0.1514 | G_loss: 1.7945\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-004200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9917000532150269, 0.9999348521232605]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [4400/20000] | D_real_loss: 0.3218 | D_fake_loss: 0.1119 | G_loss: 2.2661\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-004400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9970255494117737, 0.9999658465385437]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [4600/20000] | D_real_loss: 0.0967 | D_fake_loss: 0.1975 | G_loss: 1.9820\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-004600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9950771927833557, 0.9999449253082275]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [4800/20000] | D_real_loss: 0.0802 | D_fake_loss: 0.3158 | G_loss: 2.2060\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-004800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9953150153160095, 0.9999607801437378]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [5000/20000] | D_real_loss: 0.3308 | D_fake_loss: 0.1171 | G_loss: 1.7522\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-005000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9914634227752686, 0.9999940395355225]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [5200/20000] | D_real_loss: 0.0855 | D_fake_loss: 0.2758 | G_loss: 1.6409\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-005200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9954964518547058, 0.9999979734420776]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [5400/20000] | D_real_loss: 0.0784 | D_fake_loss: 0.1912 | G_loss: 2.2729\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-005400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9972994327545166, 0.9999975562095642]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [5600/20000] | D_real_loss: 0.2817 | D_fake_loss: 0.0889 | G_loss: 2.4096\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-005600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9934132695198059, 0.999996542930603]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [5800/20000] | D_real_loss: 0.0940 | D_fake_loss: 0.2075 | G_loss: 1.9867\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-005800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9945569038391113, 0.9999696016311646]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [6000/20000] | D_real_loss: 0.1560 | D_fake_loss: 0.1133 | G_loss: 2.2605\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-006000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9950520992279053, 0.9999944567680359]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [6200/20000] | D_real_loss: 0.0393 | D_fake_loss: 0.4145 | G_loss: 1.9720\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-006200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9976508617401123, 0.9999726414680481]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [6400/20000] | D_real_loss: 0.1950 | D_fake_loss: 0.1468 | G_loss: 2.2625\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-006400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9987343549728394, 0.9999988675117493]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [6600/20000] | D_real_loss: 0.0280 | D_fake_loss: 0.2802 | G_loss: 2.6153\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-006600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9955354928970337, 0.9999816417694092]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [6800/20000] | D_real_loss: 0.0876 | D_fake_loss: 0.2307 | G_loss: 2.1985\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-006800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9948407411575317, 0.9999934434890747]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [7000/20000] | D_real_loss: 0.0288 | D_fake_loss: 0.1711 | G_loss: 1.9439\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-007000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9974492788314819, 0.9999856948852539]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [7200/20000] | D_real_loss: 0.0936 | D_fake_loss: 0.2385 | G_loss: 1.9098\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-007200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9972378015518188, 0.9999454021453857]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [7400/20000] | D_real_loss: 0.1994 | D_fake_loss: 0.0719 | G_loss: 2.4068\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-007400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9966069459915161, 0.9999997615814209]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [7600/20000] | D_real_loss: 0.1282 | D_fake_loss: 0.1165 | G_loss: 2.1218\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-007600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9982551336288452, 0.9999967813491821]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [7800/20000] | D_real_loss: 0.0641 | D_fake_loss: 0.0860 | G_loss: 2.7229\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-007800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.987465500831604, 0.9999999403953552]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [8000/20000] | D_real_loss: 0.0090 | D_fake_loss: 0.0276 | G_loss: 3.6576\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-008000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9904460310935974, 0.9999975562095642]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [8200/20000] | D_real_loss: 0.0065 | D_fake_loss: 0.0169 | G_loss: 4.1296\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-008200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9959577322006226, 0.9998369216918945]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [8400/20000] | D_real_loss: 0.0049 | D_fake_loss: 0.0099 | G_loss: 4.6153\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-008400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9968656301498413, 0.9995129704475403]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [8600/20000] | D_real_loss: 0.0040 | D_fake_loss: 0.0075 | G_loss: 4.9115\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-008600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9968366026878357, 0.9990553259849548]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [8800/20000] | D_real_loss: 0.0068 | D_fake_loss: 0.0060 | G_loss: 5.1411\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-008800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9960436224937439, 0.9988674521446228]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [9000/20000] | D_real_loss: 0.0297 | D_fake_loss: 0.0055 | G_loss: 5.1795\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-009000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9948031306266785, 0.9990015625953674]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [9200/20000] | D_real_loss: 0.0020 | D_fake_loss: 0.0048 | G_loss: 5.3442\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-009200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9961561560630798, 0.9990676045417786]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [9400/20000] | D_real_loss: 0.0033 | D_fake_loss: 0.0040 | G_loss: 5.5275\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-009400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9957806468009949, 0.9991388320922852]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [9600/20000] | D_real_loss: 0.0020 | D_fake_loss: 0.0041 | G_loss: 5.5236\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-009600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9946140646934509, 0.9992373585700989]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [9800/20000] | D_real_loss: 0.0016 | D_fake_loss: 0.0040 | G_loss: 5.5348\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-009800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9929907321929932, 0.999254047870636]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [10000/20000] | D_real_loss: 0.0018 | D_fake_loss: 0.0033 | G_loss: 5.7444\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-010000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9915163516998291, 0.999380350112915]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [10200/20000] | D_real_loss: 0.0244 | D_fake_loss: 0.0039 | G_loss: 5.5261\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-010200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9874676465988159, 0.9996931552886963]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [10400/20000] | D_real_loss: 0.0015 | D_fake_loss: 0.0031 | G_loss: 5.8073\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-010400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.984815239906311, 0.9997056126594543]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [10600/20000] | D_real_loss: 0.0059 | D_fake_loss: 0.0030 | G_loss: 5.8176\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-010600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.977195680141449, 0.9996888041496277]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [10800/20000] | D_real_loss: 0.0021 | D_fake_loss: 0.0034 | G_loss: 5.7243\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-010800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9758267402648926, 0.9995940923690796]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [11000/20000] | D_real_loss: 0.0635 | D_fake_loss: 0.0050 | G_loss: 5.1597\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-011000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9904880523681641, 0.9990514516830444]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [11200/20000] | D_real_loss: 0.0011 | D_fake_loss: 0.0039 | G_loss: 5.6088\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-011200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9940217137336731, 0.9991662502288818]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [11400/20000] | D_real_loss: 0.0311 | D_fake_loss: 0.0069 | G_loss: 5.0971\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-011400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9998897910118103, 0.9965765476226807]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [11600/20000] | D_real_loss: 0.0014 | D_fake_loss: 0.0049 | G_loss: 5.3587\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-011600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9988713264465332, 0.9999995827674866]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [11800/20000] | D_real_loss: 0.2921 | D_fake_loss: 0.1819 | G_loss: 1.8197\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-011800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9995718002319336, 1.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [12000/20000] | D_real_loss: 0.0221 | D_fake_loss: 0.0252 | G_loss: 3.7442\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-012000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9996207356452942, 1.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [12200/20000] | D_real_loss: 0.0024 | D_fake_loss: 0.0512 | G_loss: 3.0805\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-012200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9907118678092957, 1.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [12400/20000] | D_real_loss: 0.0046 | D_fake_loss: 0.0084 | G_loss: 4.8273\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-012400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9881170392036438, 0.999938428401947]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [12600/20000] | D_real_loss: 0.0044 | D_fake_loss: 0.0063 | G_loss: 5.0791\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-012600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9922540187835693, 1.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [12800/20000] | D_real_loss: 0.0145 | D_fake_loss: 0.0102 | G_loss: 4.6171\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-012800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9983442425727844, 0.9999998807907104]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [13000/20000] | D_real_loss: 0.0083 | D_fake_loss: 0.0180 | G_loss: 3.9862\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-013000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9976456761360168, 0.9999932050704956]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [13200/20000] | D_real_loss: 0.0055 | D_fake_loss: 0.0333 | G_loss: 3.4936\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-013200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9980727434158325, 0.9999991059303284]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [13400/20000] | D_real_loss: 0.0092 | D_fake_loss: 0.0276 | G_loss: 3.6294\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-013400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9973347187042236, 0.9999586343765259]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [13600/20000] | D_real_loss: 0.0082 | D_fake_loss: 0.0248 | G_loss: 3.7759\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-013600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9953144788742065, 0.9999963641166687]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [13800/20000] | D_real_loss: 0.0140 | D_fake_loss: 0.0717 | G_loss: 2.8174\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-013800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9932119250297546, 0.9999883770942688]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [14000/20000] | D_real_loss: 0.0323 | D_fake_loss: 0.0532 | G_loss: 3.0120\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-014000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9906150698661804, 0.999981701374054]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [14200/20000] | D_real_loss: 0.0142 | D_fake_loss: 0.0275 | G_loss: 3.5772\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-014200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9962233901023865, 0.9999919533729553]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [14400/20000] | D_real_loss: 0.0114 | D_fake_loss: 0.0295 | G_loss: 3.5290\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-014400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9985978007316589, 1.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [14600/20000] | D_real_loss: 0.0966 | D_fake_loss: 0.5614 | G_loss: 2.3978\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-014600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9986180067062378, 0.9999999403953552]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [14800/20000] | D_real_loss: 0.0330 | D_fake_loss: 0.2029 | G_loss: 1.9532\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-014800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9996755123138428, 0.9999994039535522]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [15000/20000] | D_real_loss: 0.2238 | D_fake_loss: 0.2335 | G_loss: 1.5306\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-015000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9999517798423767, 0.999995768070221]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [15200/20000] | D_real_loss: 0.5049 | D_fake_loss: 0.1109 | G_loss: 2.3973\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-015200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.999804675579071, 0.9999966025352478]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [15400/20000] | D_real_loss: 0.0118 | D_fake_loss: 0.0243 | G_loss: 3.7223\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-015400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9998599290847778, 0.9999914765357971]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [15600/20000] | D_real_loss: 0.0090 | D_fake_loss: 0.0204 | G_loss: 3.9821\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-015600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9998553991317749, 0.9999818205833435]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [15800/20000] | D_real_loss: 0.0107 | D_fake_loss: 0.0189 | G_loss: 4.0191\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-015800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9999444484710693, 0.9999613165855408]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [16000/20000] | D_real_loss: 0.0074 | D_fake_loss: 0.0141 | G_loss: 4.2466\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-016000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.999947726726532, 0.9999651312828064]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [16200/20000] | D_real_loss: 0.0101 | D_fake_loss: 0.0153 | G_loss: 4.2145\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-016200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9998737573623657, 0.9999814033508301]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [16400/20000] | D_real_loss: 0.0056 | D_fake_loss: 0.0230 | G_loss: 3.8627\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-016400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9996574521064758, 0.9999963641166687]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [16600/20000] | D_real_loss: 0.0177 | D_fake_loss: 0.0237 | G_loss: 3.8467\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-016600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9999252557754517, 0.9999983906745911]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [16800/20000] | D_real_loss: 0.2309 | D_fake_loss: 0.3144 | G_loss: 2.4788\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-016800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9986923336982727, 0.9999984502792358]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [17000/20000] | D_real_loss: 0.0173 | D_fake_loss: 0.2369 | G_loss: 2.4849\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-017000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9996850490570068, 0.9999991059303284]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [17200/20000] | D_real_loss: 0.0796 | D_fake_loss: 0.2630 | G_loss: 1.7076\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-017200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9998477101325989, 0.9999834299087524]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [17400/20000] | D_real_loss: 0.1160 | D_fake_loss: 0.1021 | G_loss: 2.6133\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-017400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9995701909065247, 0.9999956488609314]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [17600/20000] | D_real_loss: 0.0296 | D_fake_loss: 0.1390 | G_loss: 1.8304\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-017600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9994966983795166, 0.999996542930603]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [17800/20000] | D_real_loss: 0.0579 | D_fake_loss: 0.1205 | G_loss: 2.1159\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-017800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9994040727615356, 0.9999932050704956]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [18000/20000] | D_real_loss: 0.4467 | D_fake_loss: 0.0576 | G_loss: 1.1562\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-018000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9989442229270935, 0.9999979138374329]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [18200/20000] | D_real_loss: 0.0516 | D_fake_loss: 0.1526 | G_loss: 2.1894\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-018200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9999471306800842, 0.9999986886978149]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [18400/20000] | D_real_loss: 0.0404 | D_fake_loss: 0.1061 | G_loss: 2.4242\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-018400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9994257092475891, 0.9999969005584717]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [18600/20000] | D_real_loss: 0.0367 | D_fake_loss: 0.3612 | G_loss: 1.5333\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-018600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9983865022659302, 0.9999999403953552]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [18800/20000] | D_real_loss: 0.0118 | D_fake_loss: 0.0226 | G_loss: 3.8356\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-018800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9952203631401062, 0.9999998211860657]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [19000/20000] | D_real_loss: 0.0100 | D_fake_loss: 0.0150 | G_loss: 4.2369\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-019000.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.995215892791748, 0.9999992251396179]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [19200/20000] | D_real_loss: 0.0111 | D_fake_loss: 0.0121 | G_loss: 4.4480\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-019200.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9962831735610962, 0.9999982118606567]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [19400/20000] | D_real_loss: 0.0074 | D_fake_loss: 0.0083 | G_loss: 4.8080\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-019400.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9971327185630798, 0.9999966025352478]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [19600/20000] | D_real_loss: 0.0054 | D_fake_loss: 0.0075 | G_loss: 4.9083\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-019600.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9981914758682251, 0.9999955892562866]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [19800/20000] | D_real_loss: 0.0052 | D_fake_loss: 0.0060 | G_loss: 5.1371\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-019800.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9984244108200073, 0.9999922513961792]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration [20000/20000] | D_real_loss: 0.0027 | D_fake_loss: 0.0055 | G_loss: 5.2226\n",
            "Saved results/samples_gan_gp1_lr3e-5/sample-020000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/losses.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-000200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-000400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-000600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-000800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-001000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-001200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-001400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-001600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-001800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-002000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-002200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-002400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-002600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-002800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-003000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-003200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-003400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-003600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-003800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-004000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-004200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-004400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-004600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-004800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-005000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-005200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-005400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-005600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-005800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-006000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-006200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-006400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-006600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-006800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-007000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-007200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-007400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-007600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-007800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-008000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-008200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-008400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-008600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-008800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-009000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-009200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-009400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-009600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-009800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-010000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-010200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-010400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-010600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-010800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-011000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-011200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-011400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-011600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-011800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-012000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-012200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-012400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-012600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-012800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-013000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-013200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-013400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-013600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-013800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-014000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-014200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-014400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-014600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-014800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-015000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-015200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-015400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-015600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-015800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-016000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-016200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-016400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-016600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-016800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-017000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-017200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-017400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-017600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-017800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-018000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-018200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-018400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-018600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-018800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-019000.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-019200.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-019400.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-019600.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-019800.png\n",
            "adding image results/samples_gan_gp1_lr3e-5/sample-020000.png\n"
          ]
        }
      ],
      "source": [
        "SEED = 11\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'image_size':32, \n",
        "              'g_conv_dim':32, \n",
        "              'd_conv_dim':64,\n",
        "              'noise_size':100,\n",
        "              'num_workers': 0,\n",
        "              'train_iters':20000,\n",
        "              'X':'Apple',  # options: 'Windows' / 'Apple'\n",
        "              'Y': None,\n",
        "              'lr':0.00003,\n",
        "              'beta1':0.5,\n",
        "              'beta2':0.999,\n",
        "              'batch_size':32, \n",
        "              'checkpoint_dir': 'results/checkpoints_gan_gp1_lr3e-5',\n",
        "              'sample_dir': 'results/samples_gan_gp1_lr3e-5',\n",
        "              'load': None,\n",
        "              'log_step':200,\n",
        "              'sample_every':200,\n",
        "              'checkpoint_every':1000,\n",
        "              'spectral_norm': False,\n",
        "              'gradient_penalty': True,\n",
        "              'least_squares_gan': False,\n",
        "              'd_train_iters': 1\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "G, D = train(args)\n",
        "\n",
        "generate_gif(\"results/samples_gan_gp1_lr3e-5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU5XajxuhQ4c"
      },
      "source": [
        "## Download your output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PqF_oI4RAM8I",
        "outputId": "53b3f8c6-6653-4088-8988-af4e177e0211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/ (stored 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-007200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-007600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-014400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-005400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-005000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-017200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-005800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-014600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/anim.gif (deflated 2%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-005600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-010000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-009000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-018200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-010200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-012600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-000600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-009600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-015200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-003400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-019600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-016600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-014800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-013600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-002800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-005200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-012400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-014000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-010400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-002000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-010600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-012200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-011000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-004200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-013000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-017800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-012800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-020000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-008400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-009400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-001600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-006200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-000800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-012000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-002400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-006000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-019400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-003200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-001400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-016000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-013800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-004400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-019200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-018600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-007800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-019800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-018000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-011800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-007400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-015400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-017000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-011200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/losses.png (deflated 3%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-001000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-000200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-016400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-016800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-011400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-002200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-008200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-008600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-018400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-009200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-003000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-003800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-013400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-008800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-004000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-013200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-014200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-001800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-007000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-006600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-009800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-000400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-017400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-001200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-006800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-018800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-004600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-004800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-008000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-003600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-015800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-019000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-010800.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-015600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-017600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-006400.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-011600.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-016200.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-015000.png (deflated 0%)\n",
            "updating: content/csc413/a4/results/samples_gan_gp1_lr3e-5/sample-002600.png (deflated 0%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e63d6f03-d77d-4757-8a5a-aa3031fd7525\", \"samples.zip\", 31711968)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!zip -r /content/csc413/a4/results/samples.zip /content/csc413/a4/results/samples_gan_gp1_lr3e-5\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/csc413/a4/results/samples.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9DaTdRNuUra7",
        "u0KxX0sDpXKb",
        "m1E_jDaBLT1P"
      ],
      "name": "Copy of a4_dcgan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
